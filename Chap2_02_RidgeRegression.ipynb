{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap2_02_RidgeRegression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUDviPkJ5RD10CtOu+ao3T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zfukuoka/Training-KikaiGakusyuZukan/blob/master/Chap2_02_RidgeRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF-onuV8stUw",
        "colab_type": "text"
      },
      "source": [
        "# 第２章  教師あり学習\n",
        "\n",
        "## 02 正則化\n",
        "\n",
        "ポイント\n",
        "\n",
        "* 正則化は過学習を防ぐための手法の一つで線形回帰のアルゴリズムと共に利用\n",
        "* テキストでは y = sin(2 * pi * x) に乱数を加えて生成したデータを元に学習に用いたデータと検証データを分けたうえで、学習誤差と検証誤差をまとめた例で説明\n",
        "  * 線形回帰の次元を増やしていくと学習誤差は小さくなる一方、検証誤差は大きくなる結果\n",
        "\n",
        "| 次数 | 学習誤差 | 検証誤差 |\n",
        "| :--- | :---: | :---: |\n",
        "| 1 | 0.412 | 0.618 |\n",
        "| 2 | 0.176 | 0.193 |\n",
        "| 3 | 0.081 | 0.492 |\n",
        "|   |       |       |\n",
        "| 6 | 0.024 | 3.472 | \n",
        "\n",
        "* 上記の事例に正則化したら、下記のようになったという結果\n",
        "\n",
        "| 次数 | 学習誤差 | 検証誤差 |\n",
        "| :--- | :---: | :---: |\n",
        "| 1 | 0.412 | 0.612 |\n",
        "| 2 | 0.372 | 0.532 |\n",
        "| 3 | 0.301 | 0.394 |\n",
        "|   |       |       |\n",
        "| 6 | 0.159 | 0.331 |\n",
        "\n",
        "* 複雑なモデル（次数が高いモデル）が過学習する様子と過学習を防げる様子は上記の事例で確認\n",
        "  * 過学習の原因として、学習パラメータ w1 が極端に大きい（または小さい）値を取ることがある\n",
        "\n",
        "| 次数 | w0 | w1 | w2 | w3 | w4 | w5 | w6 |\n",
        "| :--- | ---: | ---: | ---: | ---: | ---: | ---: | ---: |\n",
        "| 1 | -0.007 | -0.217 |        |        | | | |\n",
        "| 2 |  0.978 | -5.222 |  4.204 |        | | | |\n",
        "| 3 |  0.281 |  4.927 |-17.639 | 12.157 | | | |\n",
        "| 6 |  1.080 |-26.324 |287.431 |-1024.141|1611.144|-1147.946|308.643|\n",
        "\n",
        "* 上記の事例に正則化したら、下記のようになったという結果\n",
        "\n",
        "| 次数 | w0 | w1 | w2 | w3 | w4 | w5 | w6 |\n",
        "| :--- | ---: | ---: | ---: | ---: | ---: | ---: | ---: |\n",
        "| 1 | -0.055 | -0.149 |        |        | | | |\n",
        "| 2 | -0.066 | -0.493 |  0.421 |        | | | |\n",
        "| 3 | -0.001 | -0.716 | -0.042 |  0.670 | | | |\n",
        "| 6 |  0.191 | -0.751 | -0.497 | -0.182|  0.109 |  0.370 |  0.607|\n",
        "\n",
        "* 正則化で学習パラメータを抑えることができるのかRidge回帰の誤差関数（2次関数の場合）\n",
        "  * 右辺の第1項は線形回帰の損失関数\n",
        "  * 右辺の第2講は罰則項（または正規化項）と呼ばれるもので、学習パラメータの2乗和\n",
        "    * 切片であるw0 は罰則項に含まないのが一般的\n",
        "    * α( >= 0) は正規化の強さをコントロールするパラメータでαが大きいほど学習パラメータが抑えられ、αが小さいほど学習データへの当てはまりが強くなる\n",
        "  * Ridge回帰の損失関数R(w)を最小化するには...\n",
        "    * 右辺の第1項だけに注目すると、学習データy との誤差を小さくするような任意の w0, w1, w2 を問題になる\n",
        "    * しかし、右辺の第2項の罰則項は学習パラメータの2乗和になっているので、学習パラメータの絶対値が大きいと、損失関数全体が大きくなってしまう\n",
        "      * 結局のところ罰則項は学習パラメータの絶対値が大きくなることに対して損失が大きくなるような罰則を与える役割を持っており、この役割で学習パラメータを抑えている\n",
        "\n",
        "$$\n",
        "R(w) = \n",
        "\\sum_{i=1}^{n} \n",
        "  \\Bigl( \n",
        "    y_i\n",
        "    \\bigl(\n",
        "      w_0 + w_1 x_i + w_2 x_i^2\n",
        "    \\bigr)\n",
        "  \\Bigr)^2 + \\alpha \\bigl(\n",
        "    w_1^2 + w_2^2\n",
        "  \\bigr)\n",
        "$$\n",
        "\n",
        "\n",
        "* サンプルコードのポイントはあとで書く"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwpBoKBfsNub",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "af6aa51b-54a2-4fc0-9fa1-855e85b028a0"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "train_size = 20\n",
        "test_size = 12\n",
        "train_X = np.random.uniform(low=0.0, high=1.2, size=train_size)\n",
        "test_X = np.random.uniform(low=0.1, high=1.3, size=test_size)\n",
        "train_y = np.sin(train_X * 2 * np.pi) + np.random.normal(0.0, 0.2, train_size)\n",
        "test_y = np.sin(test_X * 2 * np.pi) + np.random.normal(0.0, 0.2, test_size)\n",
        "\n",
        "poly = PolynomialFeatures(6) #次数は6\n",
        "train_poly_X = poly.fit_transform(train_X.reshape(train_size, 1))\n",
        "test_poly_X = poly.fit_transform(test_X.reshape(test_size, 1))\n",
        "\n",
        "model = Ridge(alpha=1.0)\n",
        "model.fit(train_poly_X, train_y)\n",
        "train_pred_y = model.predict(train_poly_X)\n",
        "test_pred_y = model.predict(test_poly_X)\n",
        "\n",
        "print(mean_squared_error(train_pred_y, train_y))\n",
        "print(mean_squared_error(test_pred_y, test_y))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.19363341191787983\n",
            "0.4440375523239954\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}