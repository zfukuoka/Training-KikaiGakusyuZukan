{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap2_08_NeuralNetwork.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO9H92nToJaHEZkijKHedTs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zfukuoka/Training-KikaiGakusyuZukan/blob/master/Chap2_08_NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCInfS7gHCox",
        "colab_type": "text"
      },
      "source": [
        "# 第２章教師あり学習\n",
        "\n",
        "## 08 ニューラルネットワーク\n",
        "\n",
        "ポイント\n",
        "* やり方そのものは、今までに見てきた sklearn と同じで空のモデルを作ってから、fit()で学習し、predict()で予測させる方法\n",
        "* MLPClassifier は 「Multi-layer Perceptron classifier.」の略らしい\n",
        "* 空のモデル生成時に hidden_layer_size を渡しているが、これ以外に学習率に関係するパラメータ learning_rate, learning_rate_initや verbose というのがあった"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "991qG293FHnD",
        "colab_type": "code",
        "outputId": "adf20e83-339c-47a9-a1a5-21297e1a1928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# データ読み込み\n",
        "data = load_digits()\n",
        "\n",
        "X = data.images.reshape(len(data.images), -1)\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = MLPClassifier(hidden_layer_sizes=(16,))\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy_score(y_pred, y_test)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9555555555555556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMIR9eqn5g8V",
        "colab_type": "text"
      },
      "source": [
        "可視化として、学習による変化を見たいが、上記の場合 fit() を呼び出して学習している過程の見えるかができるか不明。この点をあとで調査。\n",
        "verbose を True にすると、下記のように繰り返し過程のログ出力を確認できた。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDjD9XBa8jnX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3147c636-7c74-4679-8669-518691b2869c"
      },
      "source": [
        "model2 = MLPClassifier(hidden_layer_sizes=(16,), verbose=True)\n",
        "model2.fit(X_train, y_train)\n",
        "y_pred2 = model2.predict(X_test)\n",
        "accuracy_score(y_pred2, y_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 13.80972310\n",
            "Iteration 2, loss = 10.50580209\n",
            "Iteration 3, loss = 8.29204241\n",
            "Iteration 4, loss = 6.64375450\n",
            "Iteration 5, loss = 5.33444629\n",
            "Iteration 6, loss = 4.43517819\n",
            "Iteration 7, loss = 3.76822411\n",
            "Iteration 8, loss = 3.19245675\n",
            "Iteration 9, loss = 2.75664386\n",
            "Iteration 10, loss = 2.42384389\n",
            "Iteration 11, loss = 2.16991050\n",
            "Iteration 12, loss = 1.96546315\n",
            "Iteration 13, loss = 1.80248228\n",
            "Iteration 14, loss = 1.66334443\n",
            "Iteration 15, loss = 1.55103536\n",
            "Iteration 16, loss = 1.45269194\n",
            "Iteration 17, loss = 1.36400208\n",
            "Iteration 18, loss = 1.28949009\n",
            "Iteration 19, loss = 1.22445818\n",
            "Iteration 20, loss = 1.16302980\n",
            "Iteration 21, loss = 1.10905361\n",
            "Iteration 22, loss = 1.05916468\n",
            "Iteration 23, loss = 1.01333910\n",
            "Iteration 24, loss = 0.97018732\n",
            "Iteration 25, loss = 0.93150058\n",
            "Iteration 26, loss = 0.89263819\n",
            "Iteration 27, loss = 0.85894085\n",
            "Iteration 28, loss = 0.82656014\n",
            "Iteration 29, loss = 0.79439336\n",
            "Iteration 30, loss = 0.76622108\n",
            "Iteration 31, loss = 0.73778996\n",
            "Iteration 32, loss = 0.71292441\n",
            "Iteration 33, loss = 0.68953183\n",
            "Iteration 34, loss = 0.66550351\n",
            "Iteration 35, loss = 0.64329451\n",
            "Iteration 36, loss = 0.62247828\n",
            "Iteration 37, loss = 0.60357638\n",
            "Iteration 38, loss = 0.58436857\n",
            "Iteration 39, loss = 0.56633876\n",
            "Iteration 40, loss = 0.55031184\n",
            "Iteration 41, loss = 0.53561731\n",
            "Iteration 42, loss = 0.52025358\n",
            "Iteration 43, loss = 0.50618353\n",
            "Iteration 44, loss = 0.49363280\n",
            "Iteration 45, loss = 0.48019121\n",
            "Iteration 46, loss = 0.46753035\n",
            "Iteration 47, loss = 0.45691958\n",
            "Iteration 48, loss = 0.44541514\n",
            "Iteration 49, loss = 0.43442844\n",
            "Iteration 50, loss = 0.42429035\n",
            "Iteration 51, loss = 0.41540052\n",
            "Iteration 52, loss = 0.40436049\n",
            "Iteration 53, loss = 0.39510526\n",
            "Iteration 54, loss = 0.38646607\n",
            "Iteration 55, loss = 0.37820964\n",
            "Iteration 56, loss = 0.37078410\n",
            "Iteration 57, loss = 0.36319089\n",
            "Iteration 58, loss = 0.35587668\n",
            "Iteration 59, loss = 0.34810988\n",
            "Iteration 60, loss = 0.34168925\n",
            "Iteration 61, loss = 0.33545184\n",
            "Iteration 62, loss = 0.32854612\n",
            "Iteration 63, loss = 0.32151951\n",
            "Iteration 64, loss = 0.31596048\n",
            "Iteration 65, loss = 0.30991207\n",
            "Iteration 66, loss = 0.30401227\n",
            "Iteration 67, loss = 0.29869120\n",
            "Iteration 68, loss = 0.29343986\n",
            "Iteration 69, loss = 0.28784897\n",
            "Iteration 70, loss = 0.28366766\n",
            "Iteration 71, loss = 0.27847591\n",
            "Iteration 72, loss = 0.27352301\n",
            "Iteration 73, loss = 0.26975716\n",
            "Iteration 74, loss = 0.26438787\n",
            "Iteration 75, loss = 0.25945014\n",
            "Iteration 76, loss = 0.25577586\n",
            "Iteration 77, loss = 0.25166962\n",
            "Iteration 78, loss = 0.24836287\n",
            "Iteration 79, loss = 0.24402906\n",
            "Iteration 80, loss = 0.23988056\n",
            "Iteration 81, loss = 0.23669536\n",
            "Iteration 82, loss = 0.23290826\n",
            "Iteration 83, loss = 0.22923135\n",
            "Iteration 84, loss = 0.22546778\n",
            "Iteration 85, loss = 0.22166973\n",
            "Iteration 86, loss = 0.21765689\n",
            "Iteration 87, loss = 0.21376544\n",
            "Iteration 88, loss = 0.21140342\n",
            "Iteration 89, loss = 0.20723161\n",
            "Iteration 90, loss = 0.20353584\n",
            "Iteration 91, loss = 0.20076902\n",
            "Iteration 92, loss = 0.19859162\n",
            "Iteration 93, loss = 0.19514055\n",
            "Iteration 94, loss = 0.19195091\n",
            "Iteration 95, loss = 0.18872763\n",
            "Iteration 96, loss = 0.18589778\n",
            "Iteration 97, loss = 0.18378456\n",
            "Iteration 98, loss = 0.18064121\n",
            "Iteration 99, loss = 0.17896199\n",
            "Iteration 100, loss = 0.17563909\n",
            "Iteration 101, loss = 0.17209953\n",
            "Iteration 102, loss = 0.16938246\n",
            "Iteration 103, loss = 0.16817353\n",
            "Iteration 104, loss = 0.16501800\n",
            "Iteration 105, loss = 0.16231135\n",
            "Iteration 106, loss = 0.16061359\n",
            "Iteration 107, loss = 0.15780487\n",
            "Iteration 108, loss = 0.15640474\n",
            "Iteration 109, loss = 0.15409404\n",
            "Iteration 110, loss = 0.15164208\n",
            "Iteration 111, loss = 0.14935884\n",
            "Iteration 112, loss = 0.14706647\n",
            "Iteration 113, loss = 0.14499164\n",
            "Iteration 114, loss = 0.14345527\n",
            "Iteration 115, loss = 0.14077106\n",
            "Iteration 116, loss = 0.13929099\n",
            "Iteration 117, loss = 0.13687115\n",
            "Iteration 118, loss = 0.13674367\n",
            "Iteration 119, loss = 0.13295805\n",
            "Iteration 120, loss = 0.13273985\n",
            "Iteration 121, loss = 0.12977872\n",
            "Iteration 122, loss = 0.12871634\n",
            "Iteration 123, loss = 0.12647798\n",
            "Iteration 124, loss = 0.12507063\n",
            "Iteration 125, loss = 0.12353459\n",
            "Iteration 126, loss = 0.12254438\n",
            "Iteration 127, loss = 0.12139064\n",
            "Iteration 128, loss = 0.11889158\n",
            "Iteration 129, loss = 0.11755215\n",
            "Iteration 130, loss = 0.11667185\n",
            "Iteration 131, loss = 0.11512754\n",
            "Iteration 132, loss = 0.11221309\n",
            "Iteration 133, loss = 0.11165133\n",
            "Iteration 134, loss = 0.10999996\n",
            "Iteration 135, loss = 0.10874932\n",
            "Iteration 136, loss = 0.10654098\n",
            "Iteration 137, loss = 0.10625504\n",
            "Iteration 138, loss = 0.10470205\n",
            "Iteration 139, loss = 0.10355387\n",
            "Iteration 140, loss = 0.10228668\n",
            "Iteration 141, loss = 0.10094248\n",
            "Iteration 142, loss = 0.10005615\n",
            "Iteration 143, loss = 0.09823099\n",
            "Iteration 144, loss = 0.09745256\n",
            "Iteration 145, loss = 0.09602763\n",
            "Iteration 146, loss = 0.09569711\n",
            "Iteration 147, loss = 0.09400909\n",
            "Iteration 148, loss = 0.09338561\n",
            "Iteration 149, loss = 0.09213537\n",
            "Iteration 150, loss = 0.09116614\n",
            "Iteration 151, loss = 0.09003727\n",
            "Iteration 152, loss = 0.08967995\n",
            "Iteration 153, loss = 0.08774628\n",
            "Iteration 154, loss = 0.08714219\n",
            "Iteration 155, loss = 0.08653739\n",
            "Iteration 156, loss = 0.08523354\n",
            "Iteration 157, loss = 0.08420083\n",
            "Iteration 158, loss = 0.08301406\n",
            "Iteration 159, loss = 0.08252259\n",
            "Iteration 160, loss = 0.08156688\n",
            "Iteration 161, loss = 0.08040461\n",
            "Iteration 162, loss = 0.07915999\n",
            "Iteration 163, loss = 0.07857246\n",
            "Iteration 164, loss = 0.07798136\n",
            "Iteration 165, loss = 0.07643099\n",
            "Iteration 166, loss = 0.07642006\n",
            "Iteration 167, loss = 0.07544300\n",
            "Iteration 168, loss = 0.07457911\n",
            "Iteration 169, loss = 0.07364748\n",
            "Iteration 170, loss = 0.07300900\n",
            "Iteration 171, loss = 0.07204561\n",
            "Iteration 172, loss = 0.07122735\n",
            "Iteration 173, loss = 0.07049128\n",
            "Iteration 174, loss = 0.06961748\n",
            "Iteration 175, loss = 0.06870508\n",
            "Iteration 176, loss = 0.06796701\n",
            "Iteration 177, loss = 0.06723256\n",
            "Iteration 178, loss = 0.06685320\n",
            "Iteration 179, loss = 0.06617602\n",
            "Iteration 180, loss = 0.06601749\n",
            "Iteration 181, loss = 0.06477464\n",
            "Iteration 182, loss = 0.06447073\n",
            "Iteration 183, loss = 0.06356933\n",
            "Iteration 184, loss = 0.06275946\n",
            "Iteration 185, loss = 0.06218473\n",
            "Iteration 186, loss = 0.06143570\n",
            "Iteration 187, loss = 0.06061954\n",
            "Iteration 188, loss = 0.06021395\n",
            "Iteration 189, loss = 0.05939425\n",
            "Iteration 190, loss = 0.05872909\n",
            "Iteration 191, loss = 0.05818637\n",
            "Iteration 192, loss = 0.05784519\n",
            "Iteration 193, loss = 0.05652595\n",
            "Iteration 194, loss = 0.05626034\n",
            "Iteration 195, loss = 0.05530170\n",
            "Iteration 196, loss = 0.05569544\n",
            "Iteration 197, loss = 0.05443648\n",
            "Iteration 198, loss = 0.05433942\n",
            "Iteration 199, loss = 0.05346236\n",
            "Iteration 200, loss = 0.05267382\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9481481481481482"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}