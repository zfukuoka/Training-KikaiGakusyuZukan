{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap3_13_LDA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/13biSpWwAsB3IqMWEl8c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zfukuoka/Training-KikaiGakusyuZukan/blob/master/Chap3_13_LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ra5lICqG8phS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "2e738f8c-9dbf-46c9-c505-ed4ea12f4158"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# remove で本文以外の情報を取り除く\n",
        "data = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "max_features = 1000\n",
        "# 文章データをベクトルに変換\n",
        "tf_vectorizer = CountVectorizer(max_features=max_features, stop_words='english')\n",
        "tf = tf_vectorizer.fit_transform(data.data)\n",
        "\n",
        "n_topics = 20\n",
        "model = LatentDirichletAllocation(n_components=n_topics)\n",
        "model.fit(tf)\n",
        "\n",
        "print(model.components_)  # 各トピックが持つ単語分布\n",
        "print(model.fit_transform(tf))  # トピックで表現された文章"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[5.00000000e-02 5.00000000e-02 5.00000000e-02 ... 5.00000000e-02\n",
            "  5.00000000e-02 5.00000000e-02]\n",
            " [5.00000004e-02 5.04652364e-02 5.00000023e-02 ... 4.10470384e+01\n",
            "  5.00000003e-02 5.00000003e-02]\n",
            " [5.00000004e-02 1.72770072e+01 5.00000012e-02 ... 1.60543874e-01\n",
            "  8.38542085e+00 5.00000007e-02]\n",
            " ...\n",
            " [3.05433432e-01 3.67497087e+02 5.00000007e-02 ... 5.62060452e+00\n",
            "  4.02225622e+01 5.00000426e-02]\n",
            " [3.19681523e+00 2.61993771e+00 8.46692456e+00 ... 1.15365595e+00\n",
            "  5.00000003e-02 5.00000004e-02]\n",
            " [5.00000004e-02 8.52622368e+00 5.00000003e-02 ... 5.98228158e+01\n",
            "  1.13150715e+00 9.50944605e+01]]\n",
            "[[0.00208333 0.00208333 0.00208333 ... 0.00208333 0.00208333 0.00208333]\n",
            " [0.0025     0.24598166 0.0025     ... 0.0025     0.0025     0.0025    ]\n",
            " [0.00060241 0.0510671  0.00060241 ... 0.00060241 0.00060241 0.00060241]\n",
            " ...\n",
            " [0.00454545 0.00454545 0.00454545 ... 0.00454545 0.00454545 0.09998793]\n",
            " [0.00294118 0.00294118 0.00294118 ... 0.19721609 0.00294118 0.00294118]\n",
            " [0.00357143 0.00357143 0.00357143 ... 0.00357143 0.00357143 0.31372617]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}